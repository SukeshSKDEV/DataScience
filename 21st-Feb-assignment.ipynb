{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec4592-05e5-4fcb-a3e8-600d9ded58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "Ans : \n",
    "    It is an automated method of extracting large amount of data from websites. \n",
    "    We use requests and BeautifulSoup library for web scraping.\n",
    "    \n",
    "    Why web scraping is used :\n",
    "        Web Scraping is used for getting data. Access to relevant data, having methods to analyze it \n",
    "        and performing intelligent actions based on analysis can make a huge difference in the success \n",
    "        and growth of most businesses in the modern world.\n",
    "        \n",
    "        \n",
    "        There various reason for which web scraping is used :\n",
    "            1. Price monitoring : Organization scrape pricing and other related information from their \n",
    "               competitor website to fix optimal pricing for their product to maximize revenue.\n",
    "            2. Market Research :  Organization use web scraping to extract product data, reviews, and \n",
    "               other relevant information to perform sentiment analysis, consumer trends, and competitor analysis.\n",
    "            3. News Monitoring : Organization depenedent on daily news for their day-to-day functioning can use Web \n",
    "               Scraping to generate reports based on the daily news.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c889999-edc2-41b9-8831-ea6c9562d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What are the different methods used for Web Scraping?\n",
    "Ans :\n",
    "    1. Human copy-and-paste : The simplest form of web scraping is manually copying and pasting data from \n",
    "       web pages into a text file or spreadsheet.\n",
    "        \n",
    "    2. HTML Parsing : This method will allow you to extract data from dynamic and static pages using HTTP \n",
    "       requests. \n",
    "    \n",
    "    3. DOME Parsing : Document Object Model (DOM) is an structural reprensentation HTML Documents. It allows \n",
    "                  javascript to access HTML elements and styles to manipulate them.\n",
    "                  DOM parsers can be used to access nodes that contain information and to scrape webpage with\n",
    "                  tools like XPath.\n",
    "                \n",
    "    4. Computer Vision Web-Page Analysis : Methods like machine learning and computer vision are also used to \n",
    "                                extract data from the web pages by visual interpretation just like human do.\n",
    "        \n",
    "    5. Text Pattern Matching : Regular expression or UNIX grep command can be used to extract information \n",
    "                               from web pages.\n",
    "                  \n",
    "    6. Semantic annotation recognizing : The web pages contains metadata or semantic and annotations. They\n",
    "                                         are use to locate specific data snippet.\n",
    "                                         The annotations are organized into a semantic layer are stored and managed\n",
    "                                         seperately from the web pages, so scraper can retrieve data schema and\n",
    "                                         instruction from this layer before scraping the page.\n",
    "                \n",
    "    6. Vertical aggregation : Several companies have developed vertical specific harvesting platform.\n",
    "                              These platforms create and monitor \"bots\" for specific verticals without human\n",
    "                              intervention and no work related to a specific site. The preparation involves \n",
    "                              estalishing the knowledge base for the entire vertical and then the platform creates \n",
    "                              the bots automatically. The platform's robustness is measured by the quality of the \n",
    "                              information it retrives and its scalability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc07119-6803-4d23-aafa-d93a79b85fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "Ans: \n",
    "    Beautiful Soup is a python library that makes it easy to extract information from web pages.\n",
    "    It is used to parse HTML and XML documents.\n",
    "    Beautiful Soup provides simple methods for navigating, searching, and modifying a parse tree in\n",
    "    HTML, XML files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfaa6a2-d8e6-4a06-b57c-b51560d66800",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Why is flask used in this Web Scraping project?\n",
    "Ans :\n",
    "    Flask is a lightweight framework to build websites. We use flask to parse the website and display the \n",
    "    parsed data from website into HTML documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0dd2b1-94b3-4a82-9557-4ed87a227b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "Ans :\n",
    "    CodePipeline and AWS Elastic Beanstalk services are used in this project.\n",
    "    \n",
    "    CodePipeline :\n",
    "        AWS CodePipeline is a continuous delivery service that allows you to model, visualize, and \n",
    "        automate the steps required to release your software.\n",
    "        You can quickly model and configure the different stages of a software release process. CodePipeline \n",
    "        automates the steps required to release your software changes continuously.\n",
    "    \n",
    "    AWS Elastic Beanstalk: \n",
    "        AWS Elastic Beanstalk is an easy-to-use AWS service for deploying and scaling web applications and \n",
    "        services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
